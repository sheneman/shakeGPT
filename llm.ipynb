{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "https://github.com/sheneman/shakeGPT/blob/main/llm.ipynb",
      "authorship_tag": "ABX9TyOiNpKyuMf1sgTI5qAakUvS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sheneman/shakeGPT/blob/main/llm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "dDgN8R5S4a5E"
      },
      "outputs": [],
      "source": [
        "# @title shakeGPT - A Transformer Model\n",
        "###########################################################\n",
        "#\n",
        "# ShakeGPT\n",
        "#\n",
        "# Learning to emit endless Shakespeare to demonstrate how\n",
        "# decoder-only next-token prediction transformer models\n",
        "# work\n",
        "#\n",
        "#\n",
        "# Luke Sheneman\n",
        "# sheneman@uidaho.edu\n",
        "# December 2023\n",
        "#\n",
        "# Research Computing and Data Services (RCDS)\n",
        "# Insitite for Interdisciplinary Data Sciences (IIDS)\n",
        "# University of Idaho\n",
        "#\n",
        "##########################################################\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we import our Python libraries"
      ],
      "metadata": {
        "id": "ebebtbLX_g6g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.nn import functional as F\n",
        "import torch.nn as nn\n",
        "import time\n",
        "import requests"
      ],
      "metadata": {
        "id": "u9r_mKkD_V2c"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we define a number of hyperparameters which indicate where to find our training corpus and how to train our transformer:"
      ],
      "metadata": {
        "id": "tVq_E9T0_xkH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "INPUT_CORPUS     = \"complete_shakespeare.txt\"\n",
        "INPUT_URL        = \"https://colab.research.google.com/corgiredirector?site=https%3A%2F%2Fwww.gutenberg.org%2Febooks%2F100.txt.utf-8\"\n",
        "OUTPUT_FILE      = \"AI_shakespeare_out.txt\"\n",
        "\n",
        "SEED             = int(time.time())\n",
        "TRAIN_SIZE       = 0.85\n",
        "VAL_SIZE         = 1.0 - TRAIN_SIZE\n",
        "BATCH_SIZE       = 200\n",
        "NUM_BATCHES      = 12000\n",
        "LEARNING_RATE    = 3e-4\n",
        "CONTEXT_WINDOW   = 32\n",
        "EMBEDDING_SIZE   = 64\n",
        "NUM_LAYERS       = 6\n",
        "NUM_HEADS        = 8\n",
        "FFN_HIDDEN_SIZE  = 8\n",
        "LOSS_SAMPLE_SIZE = 50"
      ],
      "metadata": {
        "id": "zzrZcO6y_o-7"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some fabulous ASCII art.   Because."
      ],
      "metadata": {
        "id": "QmplL1zaALrQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "shakespeare = \"\"\"\n",
        "   ⠀⠀⠀⠀⠀⠀⠀⠀⣀⣀⣠⣄⣀⣀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
        "   ⠀⠀⠀⠀⠀⢀⣴⠟⠛⠉⠉⠉⠉⠛⠻⣦⡀⠀⠀⠀⠀⠀⠀\n",
        "   ⠀⠀⠀⠀⢰⣿⠁⠀⠀⠀⠀⠀⠀⠀⠀⠈⣿⡆⠀⠀⠀⠀⠀\n",
        "   ⠀⠀⠀⠀⣼⣿⣦⠀⠀⠀⠀⠀⠀⠀⠀⣴⣿⣷⡀⠀⠀⠀⠀\n",
        "   ⠀⠀⠀⣰⣿⣿⣿⣤⣤⣄⠀⠀⣠⣤⣤⣿⣿⣿⣷⡀⠀⠀⠀\n",
        "   ⠀⢀⣼⣿⣿⣿⠋⢠⣤⠙⠁⠈⠋⣤⡄⠙⣿⣿⣿⣿⣄⠀⠀\n",
        "   ⢠⣿⣿⣿⣿⡿⠀⠈⠉⠀⠀⠀⠀⠉⠁⠀⢿⣿⣿⣿⣿⣷⠀\n",
        "   ⣿⣿⣿⣿⣿⣇⠀⠀⠀⠀⡀⢀⠀⠀⠀⠀⣸⣿⣿⣿⣿⣿⡆\n",
        "   ⠹⣿⣿⣿⣿⣿⠀⠀⠴⠞⠁⠈⠳⠦⠀⠀⣿⣿⣿⣿⣿⡿⠁\n",
        "   ⠀⠉⢻⡿⢿⣿⣧⠀⠀⠀⢶⡶⠀⠀⠀⣼⣿⣿⣿⡟⠋⠁⠀\n",
        "   ⠀⠀⣼⡇⠀⠀⠙⣷⣄⠀⠈⠁⠀⣠⣾⠋⠀⠀⢸⣧⠀⠀⠀\n",
        "   ⠀⠀⣿⡇⠀⠀⠀⠈⠛⠷⣶⣶⠾⠛⠁⠀⠀⠀⢸⣿⠀⠀⠀\n",
        "   ⠀⠀⢻⡇⠀⠀⠀⣀⣀⣤⣤⣤⣤⣀⣀⠀⠀⠀⢸⡟⠀⠀⠀\n",
        "   ⠀⠀⠘⣿⣴⠾⠛⠋⠉⠉⠉⠉⠉⠉⠛⠛⠷⣦⣿⠃⠀⠀⠀\n",
        "   ⠀⠀⠀⠈⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠁⠀⠀⠀⠀\n",
        "All your word are belong to us!\n",
        "\n",
        "     Shakespeare AI @ UI\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "print(shakespeare)\n"
      ],
      "metadata": {
        "id": "ycazg-oiAJpg",
        "outputId": "e4018109-f51e-4eb0-aead-9e1d37843af8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "   ⠀⠀⠀⠀⠀⠀⠀⠀⣀⣀⣠⣄⣀⣀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
            "   ⠀⠀⠀⠀⠀⢀⣴⠟⠛⠉⠉⠉⠉⠛⠻⣦⡀⠀⠀⠀⠀⠀⠀\n",
            "   ⠀⠀⠀⠀⢰⣿⠁⠀⠀⠀⠀⠀⠀⠀⠀⠈⣿⡆⠀⠀⠀⠀⠀\n",
            "   ⠀⠀⠀⠀⣼⣿⣦⠀⠀⠀⠀⠀⠀⠀⠀⣴⣿⣷⡀⠀⠀⠀⠀\n",
            "   ⠀⠀⠀⣰⣿⣿⣿⣤⣤⣄⠀⠀⣠⣤⣤⣿⣿⣿⣷⡀⠀⠀⠀\n",
            "   ⠀⢀⣼⣿⣿⣿⠋⢠⣤⠙⠁⠈⠋⣤⡄⠙⣿⣿⣿⣿⣄⠀⠀\n",
            "   ⢠⣿⣿⣿⣿⡿⠀⠈⠉⠀⠀⠀⠀⠉⠁⠀⢿⣿⣿⣿⣿⣷⠀\n",
            "   ⣿⣿⣿⣿⣿⣇⠀⠀⠀⠀⡀⢀⠀⠀⠀⠀⣸⣿⣿⣿⣿⣿⡆\n",
            "   ⠹⣿⣿⣿⣿⣿⠀⠀⠴⠞⠁⠈⠳⠦⠀⠀⣿⣿⣿⣿⣿⡿⠁\n",
            "   ⠀⠉⢻⡿⢿⣿⣧⠀⠀⠀⢶⡶⠀⠀⠀⣼⣿⣿⣿⡟⠋⠁⠀\n",
            "   ⠀⠀⣼⡇⠀⠀⠙⣷⣄⠀⠈⠁⠀⣠⣾⠋⠀⠀⢸⣧⠀⠀⠀\n",
            "   ⠀⠀⣿⡇⠀⠀⠀⠈⠛⠷⣶⣶⠾⠛⠁⠀⠀⠀⢸⣿⠀⠀⠀\n",
            "   ⠀⠀⢻⡇⠀⠀⠀⣀⣀⣤⣤⣤⣤⣀⣀⠀⠀⠀⢸⡟⠀⠀⠀\n",
            "   ⠀⠀⠘⣿⣴⠾⠛⠋⠉⠉⠉⠉⠉⠉⠛⠛⠷⣦⣿⠃⠀⠀⠀\n",
            "   ⠀⠀⠀⠈⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠁⠀⠀⠀⠀\n",
            "All your word are belong to us!\n",
            "\n",
            "     Shakespeare AI @ UI\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we set our random seed, check to see if we have a CUDA-capable GPU.\n",
        "\n",
        "If a GPU is detected, we use it."
      ],
      "metadata": {
        "id": "fx7exLn7AmPw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "# Set seed and device (GPU or CPU)\n",
        "#\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "print(\"DEVICE=\", device)\n",
        "print(\"\\n\")"
      ],
      "metadata": {
        "id": "PdKtPT0ZAjSt",
        "outputId": "3d4d4cbb-4f5f-4470-ef21-e2a9833c67fd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEVICE= cuda\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read the Complete Works of Shakespeare into a single string\n",
        "\n",
        "From:  Project Gutenburg\n",
        "\n",
        "https://www.gutenberg.org/ebooks/100.txt.utf-8"
      ],
      "metadata": {
        "id": "UImtEWggA52I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#print(\"Reading input...\", flush=True)\n",
        "#with open(INPUT_CORPUS, 'r', encoding='utf-8') as f:\n",
        "#    text = f.read()\n",
        "\n",
        "# Send a GET request to the URL\n",
        "response = requests.get(INPUT_URL)\n",
        "\n",
        "# Check if the request was successful\n",
        "if response.status_code == 200:\n",
        "    # Read the content as text\n",
        "    text = response.text\n",
        "else:\n",
        "    print(f\"Failed to fetch the file: HTTP {response.status_code}\")\n",
        "    exit(-1)\n",
        "\n",
        "print(\"Read the complete works of Shakespeare from Project Gutenburg\")\n",
        "print(\"Length = %d characters\" %len(text))\n",
        "\n",
        "vocab      = list(set(text))\n",
        "vocab_size = len(vocab)\n",
        "print(\"Detected vocabulary of size %d in corpus.\" %vocab_size, flush=True)\n"
      ],
      "metadata": {
        "id": "F5dOkBOEA21K",
        "outputId": "a06423de-ac53-4cc0-b8d5-16e55c64eb02",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Read the complete works of Shakespeare from Project Gutenburg\n",
            "Length = 72541 characters\n",
            "Detected vocabulary of size 96 in corpus.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our simple lookup-table tokenizer for characters."
      ],
      "metadata": {
        "id": "U45CrroABMfp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "# Tokenizer\n",
        "#\n",
        "char_to_idx  = { ch:i for i,ch in enumerate(vocab) }\n",
        "idx_to_char  = { i:ch for i,ch in enumerate(vocab) }\n",
        "string2tokens = lambda string: [char_to_idx[character] for character in string]\n",
        "tokens2string = lambda tokens: ''.join([idx_to_char[index] for index in tokens])\n",
        "\n",
        "tokenized_corpus = torch.tensor(string2tokens(text), dtype=torch.long)"
      ],
      "metadata": {
        "id": "UUxyHyr1BLVs"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Derive our TEST and VALIDATION sets from the tokenized corpus"
      ],
      "metadata": {
        "id": "lw6zLkteBbcr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_portion = int(TRAIN_SIZE*len(tokenized_corpus))\n",
        "training_data   = tokenized_corpus[:train_portion]\n",
        "validation_data = tokenized_corpus[train_portion:]"
      ],
      "metadata": {
        "id": "5rVxQsiKBWq9"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extract a batch of size BATCH_SIZE from either training or validation sets"
      ],
      "metadata": {
        "id": "tOVficfTBk3_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def batch(data):\n",
        "    selected = torch.randint(len(data) - CONTEXT_WINDOW, (BATCH_SIZE,))\n",
        "    input    = torch.stack([data[i:i+CONTEXT_WINDOW] for i in selected])\n",
        "    target   = torch.stack([data[i+1:i+CONTEXT_WINDOW+1] for i in selected])\n",
        "\n",
        "    return input.to(device), target.to(device)"
      ],
      "metadata": {
        "id": "b8XRTfe8Beui"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate the model during training to estimate average loss for the given dataset (training or validation)"
      ],
      "metadata": {
        "id": "64461quXB0yz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def sample_loss(data):\n",
        "\n",
        "    loss_tensor = torch.zeros(LOSS_SAMPLE_SIZE)\n",
        "    for i in range(LOSS_SAMPLE_SIZE):\n",
        "        input, target = batch(data)\n",
        "        _,loss = model(input, target)\n",
        "        loss_tensor[i] = loss.item()\n",
        "\n",
        "    model.train()\n",
        "    return(loss_tensor.mean())"
      ],
      "metadata": {
        "id": "ZhjSznXKBozw"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ATTENTION HEAD definition - Here is where the magic happens!"
      ],
      "metadata": {
        "id": "RQakNKRICEhU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class AttentionHead(nn.Module):\n",
        "\n",
        "    def __init__(self, headspace):\n",
        "        super().__init__()\n",
        "\n",
        "        # The Query(Q), Key(K), and Value(V) vectors are just linear tensors\n",
        "        self.query = nn.Linear(EMBEDDING_SIZE, headspace)\n",
        "        self.key   = nn.Linear(EMBEDDING_SIZE, headspace)\n",
        "        self.value = nn.Linear(EMBEDDING_SIZE, headspace)\n",
        "\n",
        "        # Causal Self-Attention:  define our triangular mask so we can't look into the FUTURE...\n",
        "        lower_diag = torch.tril(torch.ones(CONTEXT_WINDOW, CONTEXT_WINDOW))\n",
        "        self.register_buffer('causal_mask', lower_diag)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        batchsize, seqlen, embedlen  = x.shape\n",
        "        query_vector = self.query(x)\n",
        "        key_vector   = self.key(x)\n",
        "\n",
        "        attention_weights = query_vector @ key_vector.transpose(-2,-1) * torch.sqrt(torch.tensor(embedlen))\n",
        "        attention_weights = attention_weights.masked_fill(self.causal_mask[:seqlen, :seqlen] == 0, float('-inf'))\n",
        "        attention_weights = F.softmax(attention_weights, dim=-1)\n",
        "\n",
        "        # weighted sum of value vector\n",
        "        value_vector = self.value(x)\n",
        "        x = attention_weights @ value_vector\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "Nb3hUJxFB7k5"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MULTI-HEAD ATTENTION - Just a bunch of heads paying attention to different things."
      ],
      "metadata": {
        "id": "7aTkPdDeJv5m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # define NUM_HEADS attention heads, each head able to attend to only an equal-sized subset of the embedding layer\n",
        "        headspace = EMBEDDING_SIZE // NUM_HEADS\n",
        "        self.heads  = nn.ModuleList([AttentionHead(headspace) for h in range(NUM_HEADS)])\n",
        "        self.linear = nn.Linear(EMBEDDING_SIZE, EMBEDDING_SIZE)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # concatenate the output from all heads and aggregate through a single linear layer\n",
        "        x = torch.cat([head(x) for head in self.heads], dim=-1)\n",
        "        x = self.linear(x)\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "S33dmf_9CLjD"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define FEED FORWARD NETWORK (FFN) portion of the TransformerBlock"
      ],
      "metadata": {
        "id": "WM5pnuXrCeu6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class TransformerFeedFoward(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(EMBEDDING_SIZE, EMBEDDING_SIZE*FFN_HIDDEN_SIZE),\n",
        "            nn.LeakyReLU(negative_slope=0.01),\n",
        "            nn.Linear(EMBEDDING_SIZE*FFN_HIDDEN_SIZE, EMBEDDING_SIZE),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.feed_forward(x)\n",
        "\n",
        "        return(x)\n",
        "\n"
      ],
      "metadata": {
        "id": "cuL8jwGrCcem"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the TRANSFORMER BLOCK component, consisting of:\n",
        "\n",
        "    * Multiple Self-Attention Heads\n",
        "    * Feed-Forward neural networks\n",
        "    * Repeating Linear Layer Normalization layers\n"
      ],
      "metadata": {
        "id": "Cf7Tkz2MCvAV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.self_attention = MultiHeadAttention()\n",
        "        self.feed_forward   = TransformerFeedFoward()\n",
        "        self.linear_norm    = nn.LayerNorm(EMBEDDING_SIZE)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        lnorm0 = self.linear_norm(x)\n",
        "        x = x + self.self_attention(lnorm0)\n",
        "\n",
        "        lnorm1 = self.linear_norm(x)\n",
        "        x = x + self.feed_forward(lnorm1)\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "Ev3aFxS0CseV"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we define our overall transformer called \"ShakeGPT\".  \n",
        "\n",
        "Our transformer consists of:\n",
        "  1. Embedding Layer\n",
        "  2. Positional Encoding Layer\n",
        "  3. A sequence of Transformer Blocks\n",
        "  4. Layer Normalization layer\n",
        "  5. Linear Layer\n",
        "\n",
        "Loss is calculated using Cross Entropy.\n",
        "\n",
        "We also provide a method for inference from the trained model within the class.\n"
      ],
      "metadata": {
        "id": "K6oCYMsjC-eP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ShakeGPT(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.token_embedding_table    = nn.Embedding(vocab_size, EMBEDDING_SIZE)\n",
        "        self.position_embedding_table = nn.Embedding(CONTEXT_WINDOW, EMBEDDING_SIZE)\n",
        "        self.blocks                   = nn.Sequential(*[TransformerBlock() for _ in range(NUM_LAYERS)])\n",
        "        self.layernorm                = nn.LayerNorm(EMBEDDING_SIZE) # final layer norm\n",
        "        self.lm_head                  = nn.Linear(EMBEDDING_SIZE, vocab_size)\n",
        "\n",
        "    def forward(self, input, targets=None):\n",
        "        batchsize, seqlen = input.shape\n",
        "\n",
        "        tok_emb = self.token_embedding_table(input)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(seqlen, device=device))\n",
        "        x       = tok_emb + pos_emb\n",
        "        x       = self.blocks(x)\n",
        "        x       = self.layernorm(x)\n",
        "        logits  = self.lm_head(x)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            batchsize, seqlen, embedlen = logits.shape\n",
        "            logits  = logits.view(batchsize*seqlen, embedlen)\n",
        "            targets = targets.view(batchsize*seqlen)\n",
        "            loss    = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "\n",
        "    def generate(self, idx, length):\n",
        "        for i in range(length):\n",
        "            idxc         = idx[:, -CONTEXT_WINDOW:]\n",
        "            logits,_     = self(idxc)\n",
        "            logits       = logits[:, -1, :]\n",
        "            probs        = F.softmax(logits, dim=-1)\n",
        "            idx_next     = torch.multinomial(probs, num_samples=1)\n",
        "            idx          = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "        return idx"
      ],
      "metadata": {
        "id": "fMwCxss7C9Pj"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instantiate our PyTorch transformer model and send it to the GPU"
      ],
      "metadata": {
        "id": "uGZpw4ElFova"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model     = ShakeGPT().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)"
      ],
      "metadata": {
        "id": "6UXwPkI8Fm2V"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "*** TRAINING ***\n",
        "\n",
        "Train in batches of size BATCH_SIZE for NUM_BATCHES\n",
        "\n",
        "Occasionally evaluate the current model to determine training and validation loss\n"
      ],
      "metadata": {
        "id": "VdqEXXWQFwsS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for b in range(NUM_BATCHES):\n",
        "\n",
        "    # Pull a random training batch\n",
        "    input, target = batch(training_data)\n",
        "\n",
        "    # Forward pass of data through the model\n",
        "    _,loss = model(input, target)\n",
        "\n",
        "    optimizer.zero_grad() # Reset our gradients for this batch\n",
        "    loss.backward()       # Backpropagation\n",
        "    optimizer.step()      # Neural Network Optimization\n",
        "\n",
        "    # Every 100 batches, let's sample and report our loss\n",
        "    if(not b%100):\n",
        "        torch.no_grad()\n",
        "        model.eval()\n",
        "        train_loss = sample_loss(training_data)\n",
        "        val_loss   = sample_loss(validation_data)\n",
        "        print(f\"BATCH %s/%d: training loss=%.05f | validation loss=%.05f\" %(str(b).zfill(4),NUM_BATCHES,train_loss,val_loss))\n",
        "        model.train()\n",
        "\n"
      ],
      "metadata": {
        "id": "SdrUz7szFsFf",
        "outputId": "c11e9991-a090-45dd-ee9c-dca84b9a4c9a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 555
        }
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BATCH 0000/12000: training loss=4.63205 | validation loss=4.63504\n",
            "BATCH 0100/12000: training loss=3.26457 | validation loss=3.47192\n",
            "BATCH 0200/12000: training loss=2.71988 | validation loss=3.19646\n",
            "BATCH 0300/12000: training loss=2.33570 | validation loss=3.03117\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-3d5536a812bd>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Reset our gradients for this batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m       \u001b[0;31m# Backpropagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m      \u001b[0;31m# Neural Network Optimization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    490\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             )\n\u001b[0;32m--> 492\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    493\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    252\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*** INFERENCE ***\n",
        "\n",
        "Training done.  Let's spew a chunk of Shakespearish!"
      ],
      "metadata": {
        "id": "H_5a7qoUF6vw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"INFERENCE TIME.  SPEWING...(be patient)\", flush=True)\n",
        "\n",
        "f = open(OUTPUT_FILE, \"w\", encoding=\"utf-8\")\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "x = model.generate(context, length=5000)\n",
        "generated_text = tokens2string(x[0].tolist())\n",
        "print(generated_text)\n",
        "f.write(generated_text)\n",
        "f.close()"
      ],
      "metadata": {
        "id": "8IFqcH35F4om"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DONE!"
      ],
      "metadata": {
        "id": "jySPjB4rF_2I"
      }
    }
  ]
}