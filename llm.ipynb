{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "https://github.com/sheneman/shakeGPT/blob/main/llm.ipynb",
      "authorship_tag": "ABX9TyPimi5AjK4JV6DfXcE3/8AT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sheneman/shakeGPT/blob/main/llm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dDgN8R5S4a5E"
      },
      "outputs": [],
      "source": [
        "# @title shakeGPT - A Transformer Model\n",
        "###########################################################\n",
        "#\n",
        "# ShakeGPT\n",
        "#\n",
        "# Learning to emit endless Shakespeare to demonstrate how\n",
        "# decoder-only next-token prediction transformer models\n",
        "# work\n",
        "#\n",
        "#\n",
        "# Luke Sheneman\n",
        "# sheneman@uidaho.edu\n",
        "# December 2023\n",
        "#\n",
        "# Research Computing and Data Services (RCDS)\n",
        "# Insitite for Interdisciplinary Data Sciences (IIDS)\n",
        "# University of Idaho\n",
        "#\n",
        "##########################################################\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we import our Python libraries"
      ],
      "metadata": {
        "id": "ebebtbLX_g6g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.nn import functional as F\n",
        "import torch.nn as nn\n",
        "import time\n",
        "import requests"
      ],
      "metadata": {
        "id": "u9r_mKkD_V2c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we define a number of hyperparameters which indicate where to find our training corpus and how to train our transformer:"
      ],
      "metadata": {
        "id": "tVq_E9T0_xkH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "INPUT_CORPUS     = \"complete_shakespeare.txt\"\n",
        "INPUT_URL        = \"https://www.gutenberg.org/ebooks/100.txt.utf-8\"\n",
        "OUTPUT_FILE      = \"AI_shakespeare_out.txt\"\n",
        "\n",
        "SEED             = int(time.time())\n",
        "TRAIN_SIZE       = 0.85\n",
        "VAL_SIZE         = 1.0 - TRAIN_SIZE\n",
        "BATCH_SIZE       = 200\n",
        "NUM_BATCHES      = 2000\n",
        "LEARNING_RATE    = 3e-4\n",
        "CONTEXT_WINDOW   = 32\n",
        "EMBEDDING_SIZE   = 64\n",
        "NUM_LAYERS       = 6\n",
        "NUM_HEADS        = 8\n",
        "FFN_HIDDEN_SIZE  = 8\n",
        "LOSS_SAMPLE_SIZE = 50"
      ],
      "metadata": {
        "id": "zzrZcO6y_o-7"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some fabulous ASCII art.   Because."
      ],
      "metadata": {
        "id": "QmplL1zaALrQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "shakespeare = \"\"\"\n",
        "   ⠀⠀⠀⠀⠀⠀⠀⠀⣀⣀⣠⣄⣀⣀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
        "   ⠀⠀⠀⠀⠀⢀⣴⠟⠛⠉⠉⠉⠉⠛⠻⣦⡀⠀⠀⠀⠀⠀⠀\n",
        "   ⠀⠀⠀⠀⢰⣿⠁⠀⠀⠀⠀⠀⠀⠀⠀⠈⣿⡆⠀⠀⠀⠀⠀\n",
        "   ⠀⠀⠀⠀⣼⣿⣦⠀⠀⠀⠀⠀⠀⠀⠀⣴⣿⣷⡀⠀⠀⠀⠀\n",
        "   ⠀⠀⠀⣰⣿⣿⣿⣤⣤⣄⠀⠀⣠⣤⣤⣿⣿⣿⣷⡀⠀⠀⠀\n",
        "   ⠀⢀⣼⣿⣿⣿⠋⢠⣤⠙⠁⠈⠋⣤⡄⠙⣿⣿⣿⣿⣄⠀⠀\n",
        "   ⢠⣿⣿⣿⣿⡿⠀⠈⠉⠀⠀⠀⠀⠉⠁⠀⢿⣿⣿⣿⣿⣷⠀\n",
        "   ⣿⣿⣿⣿⣿⣇⠀⠀⠀⠀⡀⢀⠀⠀⠀⠀⣸⣿⣿⣿⣿⣿⡆\n",
        "   ⠹⣿⣿⣿⣿⣿⠀⠀⠴⠞⠁⠈⠳⠦⠀⠀⣿⣿⣿⣿⣿⡿⠁\n",
        "   ⠀⠉⢻⡿⢿⣿⣧⠀⠀⠀⢶⡶⠀⠀⠀⣼⣿⣿⣿⡟⠋⠁⠀\n",
        "   ⠀⠀⣼⡇⠀⠀⠙⣷⣄⠀⠈⠁⠀⣠⣾⠋⠀⠀⢸⣧⠀⠀⠀\n",
        "   ⠀⠀⣿⡇⠀⠀⠀⠈⠛⠷⣶⣶⠾⠛⠁⠀⠀⠀⢸⣿⠀⠀⠀\n",
        "   ⠀⠀⢻⡇⠀⠀⠀⣀⣀⣤⣤⣤⣤⣀⣀⠀⠀⠀⢸⡟⠀⠀⠀\n",
        "   ⠀⠀⠘⣿⣴⠾⠛⠋⠉⠉⠉⠉⠉⠉⠛⠛⠷⣦⣿⠃⠀⠀⠀\n",
        "   ⠀⠀⠀⠈⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠁⠀⠀⠀⠀\n",
        "All your word are belong to us!\n",
        "\n",
        "     Shakespeare AI @ UI\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "print(shakespeare)\n"
      ],
      "metadata": {
        "id": "ycazg-oiAJpg",
        "outputId": "00ccfaf5-64cf-4725-dc56-3648ffa092b1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "   ⠀⠀⠀⠀⠀⠀⠀⠀⣀⣀⣠⣄⣀⣀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
            "   ⠀⠀⠀⠀⠀⢀⣴⠟⠛⠉⠉⠉⠉⠛⠻⣦⡀⠀⠀⠀⠀⠀⠀\n",
            "   ⠀⠀⠀⠀⢰⣿⠁⠀⠀⠀⠀⠀⠀⠀⠀⠈⣿⡆⠀⠀⠀⠀⠀\n",
            "   ⠀⠀⠀⠀⣼⣿⣦⠀⠀⠀⠀⠀⠀⠀⠀⣴⣿⣷⡀⠀⠀⠀⠀\n",
            "   ⠀⠀⠀⣰⣿⣿⣿⣤⣤⣄⠀⠀⣠⣤⣤⣿⣿⣿⣷⡀⠀⠀⠀\n",
            "   ⠀⢀⣼⣿⣿⣿⠋⢠⣤⠙⠁⠈⠋⣤⡄⠙⣿⣿⣿⣿⣄⠀⠀\n",
            "   ⢠⣿⣿⣿⣿⡿⠀⠈⠉⠀⠀⠀⠀⠉⠁⠀⢿⣿⣿⣿⣿⣷⠀\n",
            "   ⣿⣿⣿⣿⣿⣇⠀⠀⠀⠀⡀⢀⠀⠀⠀⠀⣸⣿⣿⣿⣿⣿⡆\n",
            "   ⠹⣿⣿⣿⣿⣿⠀⠀⠴⠞⠁⠈⠳⠦⠀⠀⣿⣿⣿⣿⣿⡿⠁\n",
            "   ⠀⠉⢻⡿⢿⣿⣧⠀⠀⠀⢶⡶⠀⠀⠀⣼⣿⣿⣿⡟⠋⠁⠀\n",
            "   ⠀⠀⣼⡇⠀⠀⠙⣷⣄⠀⠈⠁⠀⣠⣾⠋⠀⠀⢸⣧⠀⠀⠀\n",
            "   ⠀⠀⣿⡇⠀⠀⠀⠈⠛⠷⣶⣶⠾⠛⠁⠀⠀⠀⢸⣿⠀⠀⠀\n",
            "   ⠀⠀⢻⡇⠀⠀⠀⣀⣀⣤⣤⣤⣤⣀⣀⠀⠀⠀⢸⡟⠀⠀⠀\n",
            "   ⠀⠀⠘⣿⣴⠾⠛⠋⠉⠉⠉⠉⠉⠉⠛⠛⠷⣦⣿⠃⠀⠀⠀\n",
            "   ⠀⠀⠀⠈⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠁⠀⠀⠀⠀\n",
            "All your word are belong to us!\n",
            "\n",
            "     Shakespeare AI @ UI\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we set our random seed, check to see if we have a CUDA-capable GPU.\n",
        "\n",
        "If a GPU is detected, we use it."
      ],
      "metadata": {
        "id": "fx7exLn7AmPw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "# Set seed and device (GPU or CPU)\n",
        "#\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "print(\"DEVICE=\", device)\n",
        "print(\"\\n\")"
      ],
      "metadata": {
        "id": "PdKtPT0ZAjSt",
        "outputId": "2bbc52ca-613a-4f7a-9c78-ccae9094fa6d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEVICE= cuda\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read the Complete Works of Shakespeare into a single string\n",
        "\n",
        "From:  Project Gutenburg\n",
        "\n",
        "https://www.gutenberg.org/ebooks/100.txt.utf-8"
      ],
      "metadata": {
        "id": "UImtEWggA52I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#print(\"Reading input...\", flush=True)\n",
        "#with open(INPUT_CORPUS, 'r', encoding='utf-8') as f:\n",
        "#    text = f.read()\n",
        "\n",
        "# Send a GET request to the URL\n",
        "response = requests.get(INPUT_URL)\n",
        "\n",
        "# Check if the request was successful\n",
        "if response.status_code == 200:\n",
        "    # Read the content as text\n",
        "    text = response.text\n",
        "else:\n",
        "    print(f\"Failed to fetch the file: HTTP {response.status_code}\")\n",
        "    exit(-1)\n",
        "\n",
        "print(\"Hey we just read the complete works of Shakespeare directly from Project Gutenburg!\")\n",
        "print(\"Length = %d characters\" %len(text))\n",
        "\n",
        "\n",
        "vocab      = list(set(text))\n",
        "vocab_size = len(vocab)\n",
        "print(\"Detected vocabulary of size %d in corpus.\" %vocab_size, flush=True)\n"
      ],
      "metadata": {
        "id": "F5dOkBOEA21K",
        "outputId": "1c4de79d-17eb-4959-f677-7af0d7ce1476",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hey we just read the complete works of Shakespeare directly from Project Gutenburg!\n",
            "Length = 5575084 characters\n",
            "Detected vocabulary of size 108 in corpus.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our simple lookup-table tokenizer for characters."
      ],
      "metadata": {
        "id": "U45CrroABMfp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "# Tokenizer\n",
        "#\n",
        "char_to_idx  = { ch:i for i,ch in enumerate(vocab) }\n",
        "idx_to_char  = { i:ch for i,ch in enumerate(vocab) }\n",
        "string2tokens = lambda string: [char_to_idx[character] for character in string]\n",
        "tokens2string = lambda tokens: ''.join([idx_to_char[index] for index in tokens])\n",
        "\n",
        "tokenized_corpus = torch.tensor(string2tokens(text), dtype=torch.long)"
      ],
      "metadata": {
        "id": "UUxyHyr1BLVs"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Derive our TEST and VALIDATION sets from the tokenized corpus"
      ],
      "metadata": {
        "id": "lw6zLkteBbcr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_portion = int(TRAIN_SIZE*len(tokenized_corpus))\n",
        "training_data   = tokenized_corpus[:train_portion]\n",
        "validation_data = tokenized_corpus[train_portion:]"
      ],
      "metadata": {
        "id": "5rVxQsiKBWq9"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extract a batch of size BATCH_SIZE from either training or validation sets"
      ],
      "metadata": {
        "id": "tOVficfTBk3_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def batch(data):\n",
        "    selected = torch.randint(len(data) - CONTEXT_WINDOW, (BATCH_SIZE,))\n",
        "    input    = torch.stack([data[i:i+CONTEXT_WINDOW] for i in selected])\n",
        "    target   = torch.stack([data[i+1:i+CONTEXT_WINDOW+1] for i in selected])\n",
        "\n",
        "    return input.to(device), target.to(device)"
      ],
      "metadata": {
        "id": "b8XRTfe8Beui"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate the model during training to estimate average loss for the given dataset (training or validation)"
      ],
      "metadata": {
        "id": "64461quXB0yz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def sample_loss(data):\n",
        "\n",
        "    loss_tensor = torch.zeros(LOSS_SAMPLE_SIZE)\n",
        "    for i in range(LOSS_SAMPLE_SIZE):\n",
        "        input, target = batch(data)\n",
        "        _,loss = model(input, target)\n",
        "        loss_tensor[i] = loss.item()\n",
        "\n",
        "    model.train()\n",
        "    return(loss_tensor.mean())"
      ],
      "metadata": {
        "id": "ZhjSznXKBozw"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ATTENTION HEAD definition - Here is where the magic happens!"
      ],
      "metadata": {
        "id": "RQakNKRICEhU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class AttentionHead(nn.Module):\n",
        "\n",
        "    def __init__(self, headspace):\n",
        "        super().__init__()\n",
        "\n",
        "        # The Query(Q), Key(K), and Value(V) vectors are just linear tensors\n",
        "        self.query = nn.Linear(EMBEDDING_SIZE, headspace)\n",
        "        self.key   = nn.Linear(EMBEDDING_SIZE, headspace)\n",
        "        self.value = nn.Linear(EMBEDDING_SIZE, headspace)\n",
        "\n",
        "        # Causal Self-Attention:  define our triangular mask so we can't look into the FUTURE...\n",
        "        lower_diag = torch.tril(torch.ones(CONTEXT_WINDOW, CONTEXT_WINDOW))\n",
        "        self.register_buffer('causal_mask', lower_diag)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        batchsize, seqlen, embedlen  = x.shape\n",
        "        query_vector = self.query(x)\n",
        "        key_vector   = self.key(x)\n",
        "\n",
        "        attention_weights = query_vector @ key_vector.transpose(-2,-1) * torch.sqrt(torch.tensor(embedlen))\n",
        "        attention_weights = attention_weights.masked_fill(self.causal_mask[:seqlen, :seqlen] == 0, float('-inf'))\n",
        "        attention_weights = F.softmax(attention_weights, dim=-1)\n",
        "\n",
        "        # weighted sum of value vector\n",
        "        value_vector = self.value(x)\n",
        "        x = attention_weights @ value_vector\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "Nb3hUJxFB7k5"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MULTI-HEAD ATTENTION - Just a bunch of heads paying attention to different things."
      ],
      "metadata": {
        "id": "7aTkPdDeJv5m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # define NUM_HEADS attention heads, each head able to attend to only an equal-sized subset of the embedding layer\n",
        "        headspace = EMBEDDING_SIZE // NUM_HEADS\n",
        "        self.heads  = nn.ModuleList([AttentionHead(headspace) for h in range(NUM_HEADS)])\n",
        "        self.linear = nn.Linear(EMBEDDING_SIZE, EMBEDDING_SIZE)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # concatenate the output from all heads and aggregate through a single linear layer\n",
        "        x = torch.cat([head(x) for head in self.heads], dim=-1)\n",
        "        x = self.linear(x)\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "S33dmf_9CLjD"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define FEED FORWARD NETWORK (FFN) portion of the TransformerBlock"
      ],
      "metadata": {
        "id": "WM5pnuXrCeu6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class TransformerFeedFoward(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(EMBEDDING_SIZE, EMBEDDING_SIZE*FFN_HIDDEN_SIZE),\n",
        "            nn.LeakyReLU(negative_slope=0.01),\n",
        "            nn.Linear(EMBEDDING_SIZE*FFN_HIDDEN_SIZE, EMBEDDING_SIZE),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.feed_forward(x)\n",
        "\n",
        "        return(x)\n",
        "\n"
      ],
      "metadata": {
        "id": "cuL8jwGrCcem"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the TRANSFORMER BLOCK component, consisting of:\n",
        "\n",
        "    * Multiple Self-Attention Heads\n",
        "    * Feed-Forward neural networks\n",
        "    * Repeating Linear Layer Normalization layers\n"
      ],
      "metadata": {
        "id": "Cf7Tkz2MCvAV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.self_attention = MultiHeadAttention()\n",
        "        self.feed_forward   = TransformerFeedFoward()\n",
        "        self.linear_norm    = nn.LayerNorm(EMBEDDING_SIZE)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        lnorm0 = self.linear_norm(x)\n",
        "        x = x + self.self_attention(lnorm0)\n",
        "\n",
        "        lnorm1 = self.linear_norm(x)\n",
        "        x = x + self.feed_forward(lnorm1)\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "Ev3aFxS0CseV"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we define our overall transformer called \"ShakeGPT\".  \n",
        "\n",
        "Our transformer consists of:\n",
        "  1. Embedding Layer\n",
        "  2. Positional Encoding Layer\n",
        "  3. A sequence of Transformer Blocks\n",
        "  4. Layer Normalization layer\n",
        "  5. Linear Layer\n",
        "\n",
        "Loss is calculated using Cross Entropy.\n",
        "\n",
        "We also provide a method for inference from the trained model within the class.\n"
      ],
      "metadata": {
        "id": "K6oCYMsjC-eP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ShakeGPT(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.token_embedding_table    = nn.Embedding(vocab_size, EMBEDDING_SIZE)\n",
        "        self.position_embedding_table = nn.Embedding(CONTEXT_WINDOW, EMBEDDING_SIZE)\n",
        "        self.blocks                   = nn.Sequential(*[TransformerBlock() for _ in range(NUM_LAYERS)])\n",
        "        self.layernorm                = nn.LayerNorm(EMBEDDING_SIZE) # final layer norm\n",
        "        self.lm_head                  = nn.Linear(EMBEDDING_SIZE, vocab_size)\n",
        "\n",
        "    def forward(self, input, targets=None):\n",
        "        batchsize, seqlen = input.shape\n",
        "\n",
        "        tok_emb = self.token_embedding_table(input)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(seqlen, device=device))\n",
        "        x       = tok_emb + pos_emb\n",
        "        x       = self.blocks(x)\n",
        "        x       = self.layernorm(x)\n",
        "        logits  = self.lm_head(x)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            batchsize, seqlen, embedlen = logits.shape\n",
        "            logits  = logits.view(batchsize*seqlen, embedlen)\n",
        "            targets = targets.view(batchsize*seqlen)\n",
        "            loss    = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "\n",
        "    def generate(self, idx, length):\n",
        "        for i in range(length):\n",
        "            idxc         = idx[:, -CONTEXT_WINDOW:]\n",
        "            logits,_     = self(idxc)\n",
        "            logits       = logits[:, -1, :]\n",
        "            probs        = F.softmax(logits, dim=-1)\n",
        "            idx_next     = torch.multinomial(probs, num_samples=1)\n",
        "            idx          = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "        return idx"
      ],
      "metadata": {
        "id": "fMwCxss7C9Pj"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instantiate our PyTorch transformer model and send it to the GPU"
      ],
      "metadata": {
        "id": "uGZpw4ElFova"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model     = ShakeGPT().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)"
      ],
      "metadata": {
        "id": "6UXwPkI8Fm2V"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "*** TRAINING ***\n",
        "\n",
        "Train in batches of size BATCH_SIZE for NUM_BATCHES\n",
        "\n",
        "Occasionally evaluate the current model to determine training and validation loss\n"
      ],
      "metadata": {
        "id": "VdqEXXWQFwsS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for b in range(NUM_BATCHES):\n",
        "\n",
        "    # Pull a random training batch\n",
        "    input, target = batch(training_data)\n",
        "\n",
        "    # Forward pass of data through the model\n",
        "    _,loss = model(input, target)\n",
        "\n",
        "    optimizer.zero_grad() # Reset our gradients for this batch\n",
        "    loss.backward()       # Backpropagation\n",
        "    optimizer.step()      # Neural Network Optimization\n",
        "\n",
        "    # Every 100 batches, let's sample and report our loss\n",
        "    if(not b%100):\n",
        "        torch.no_grad()\n",
        "        model.eval()\n",
        "        train_loss = sample_loss(training_data)\n",
        "        val_loss   = sample_loss(validation_data)\n",
        "        print(f\"BATCH %s/%d: training loss=%.05f | validation loss=%.05f\" %(str(b).zfill(4),NUM_BATCHES,train_loss,val_loss))\n",
        "        model.train()\n",
        "\n"
      ],
      "metadata": {
        "id": "SdrUz7szFsFf",
        "outputId": "cc2e755e-5075-4200-ee51-e1ae4e659e78",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BATCH 0000/2000: training loss=4.67348 | validation loss=4.66760\n",
            "BATCH 0100/2000: training loss=2.82680 | validation loss=2.83257\n",
            "BATCH 0200/2000: training loss=2.51810 | validation loss=2.54301\n",
            "BATCH 0300/2000: training loss=2.37529 | validation loss=2.40609\n",
            "BATCH 0400/2000: training loss=2.28504 | validation loss=2.31479\n",
            "BATCH 0500/2000: training loss=2.20793 | validation loss=2.24271\n",
            "BATCH 0600/2000: training loss=2.13698 | validation loss=2.17996\n",
            "BATCH 0700/2000: training loss=2.08139 | validation loss=2.12036\n",
            "BATCH 0800/2000: training loss=2.03329 | validation loss=2.07470\n",
            "BATCH 0900/2000: training loss=1.99067 | validation loss=2.03509\n",
            "BATCH 1000/2000: training loss=1.96226 | validation loss=2.00280\n",
            "BATCH 1100/2000: training loss=1.92394 | validation loss=1.97502\n",
            "BATCH 1200/2000: training loss=1.90801 | validation loss=1.95299\n",
            "BATCH 1300/2000: training loss=1.87386 | validation loss=1.92639\n",
            "BATCH 1400/2000: training loss=1.85407 | validation loss=1.90607\n",
            "BATCH 1500/2000: training loss=1.83006 | validation loss=1.88373\n",
            "BATCH 1600/2000: training loss=1.80902 | validation loss=1.87196\n",
            "BATCH 1700/2000: training loss=1.79466 | validation loss=1.85600\n",
            "BATCH 1800/2000: training loss=1.77779 | validation loss=1.84179\n",
            "BATCH 1900/2000: training loss=1.76398 | validation loss=1.83872\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*** INFERENCE ***\n",
        "\n",
        "Training done.  Let's spew a chunk of Shakespearish!"
      ],
      "metadata": {
        "id": "H_5a7qoUF6vw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"INFERENCE TIME.  SPEWING...(be patient)\", flush=True)\n",
        "\n",
        "f = open(OUTPUT_FILE, \"w\", encoding=\"utf-8\")\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "x = model.generate(context, length=5000)\n",
        "generated_text = tokens2string(x[0].tolist())\n",
        "print(generated_text)\n",
        "f.write(generated_text)\n",
        "f.close()"
      ],
      "metadata": {
        "id": "8IFqcH35F4om",
        "outputId": "21c98668-c6c0-4734-9e90-7314eec5f97f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFERENCE TIME.  SPEWING...(be patient)\n",
            "\n",
            "LANTER.\n",
            "Cack foul I would for swan windryen for man mories they call, I sives anfer laught knand.\n",
            "Gone, ight in your hones a us fell?\n",
            "Heing, that is that but odung-geis\n",
            " Edgaver of your and lay he srong coorn?\n",
            "Nem.\n",
            "I will not and they jera woe well;\n",
            "O dath wouth, he brook it with him she rabout thou lef\n",
            "FIk it twelfor miner that but of prearit\n",
            "Traster of my thus, giul, which are—Thant\n",
            "So Jownly, and Curcarr’d, that at the not cat\n",
            "ancelfits is ony your greson, head\n",
            "bechion, there this were dignotume\n",
            "Way will hear? And ost wcom ditus more of good not was for entrute bod, seecuobs fir’stim not bed a dame owring thy home, upon men you of Bultom! I prate, canscrosed,\n",
            "        That to you falEer TITON.\n",
            "’F this will talk of with yecamer\n",
            "And to bomen tu-grast ople leading’s of’ think aât macs’d\n",
            "And untogs doneshs were ulash, for oton, Lady?\n",
            "\n",
            "P(APALLO.\n",
            "He that I will my an her make I have dets and awibmaw?\n",
            "\n",
            "KINT INoth your for or spreport in poorbferice, for forther, I thank so bleat you lamly,\n",
            "And have the gain han a fry in your mans\n",
            "In reason but the did to tranother do Rome\n",
            "Shee: the is fit by this and in her daughtiin. I coonestrempast\n",
            "Sirn you od yen-band youtht nods,\n",
            "Than, it serve soome is doth at at tonitht,\n",
            "Looks, and youts chimbeis in on stend eyerp!\n",
            "That me.\n",
            "I crifived a stelf, and spever your to molught have me hope.\n",
            "\n",
            "AFESTLER.\n",
            "We’llowil, I be sewect islat of his\n",
            "I what of is gonest lord lihts. I lum fixt.\n",
            "\n",
            "FRING PANDICQURI.\n",
            "My him samplant.\n",
            "Good me twills but him; your parly\n",
            "Boy suckest Dugnied. Ly, Henquen to come.\n",
            "[_] Seand I I no conchinarestinged for’ther,\n",
            "That let us, and previorm weell, hisfood thy\n",
            "The purtence, took.\n",
            "\n",
            "KING WINOL.\n",
            "If I hils?\n",
            "\n",
            "JODEYTUS.\n",
            "Thou somell is and meereng from.”\n",
            "Yes, where I go blok: and time him,\n",
            "The hath not, a aster Warch harl. Son’sty ladres,\n",
            "Thou if sture a they, for there by shave.\n",
            "If the undeable.\n",
            "\n",
            "TICRD PEMYRIUS.\n",
            "Evont.\n",
            "So no ingeates”I lar!’ wity havly:\n",
            "INALOOM.\n",
            "Welvents may fall, love hustand his worth a Saboout\n",
            "Ohis nue??\n",
            "\n",
            "DOFPERINCH.\n",
            "A makellons, as chees in uprabent.\n",
            "\n",
            "SOLLET.\n",
            "You shupserableg’s beds, yet lo?\n",
            "\n",
            "SENBELL.\n",
            "O Antyences hounsand beas Buckfo5hout thats your hingings him botway usend infeaither,\n",
            "Of is good copontar\n",
            "Come not thy detwerI heir dight, roblek you shaming ofthongrud;\n",
            "mertistie new is to Here such?\n",
            "Promelants rome a word that thy reven, I still, such and tick,\n",
            "And say sweet ond monin of who a from.\n",
            "\n",
            "COUCUSTER.\n",
            "Nay, he now part the should lixt; be firm\n",
            "The tablour, thave is are in in thus of putawas\n",
            "VIWO wALLD.\n",
            "O Pempor you, I’r beccorsibliga!\n",
            "My man wiferling s’d a grits\n",
            "Thy boveries, herfor’s, dieeck?\n",
            "I let his made which finsbely.\n",
            "That own me. York us\n",
            "a have their sar?\n",
            "Have burt mory dright day spuage about\n",
            "In me, oy his prestire twenryamecon, shir?\n",
            "\n",
            "ETELES.\n",
            "O \n",
            "For compeien in this domany sitieng wish lied to gaunt.\n",
            "\n",
            "HER.\n",
            "I doph’ heartengest of for for plood’sd as theage bawst of world, my\n",
            "Tasserfeam awter in to wal.\n",
            "\n",
            "EDION.\n",
            "Shaless than abooners.\n",
            "\n",
            "[_I were here, thy Calsess,\n",
            "So hape thGey moresa-wit’s not the pound pouds.\n",
            "\n",
            "QUKEETEEED.\n",
            "Your slame?\n",
            "\n",
            "POSSTHER.\n",
            "I not wingPloway! Let hissonothers, incloph, be wo?\n",
            "\n",
            "ROOETR.\n",
            "I sa? fall, that her is. A and love.\n",
            ";\n",
            "QUpess Astagins. Git sclaun had it\n",
            "arrabeas’st will the\n",
            "Hend agy cust.\n",
            "Sir I stown your nevers’d my paster.\n",
            "\n",
            "MENOND ARIUS.\n",
            "Day, havth with and promstriquiantire\n",
            "Sbeave to will rechil: will unclamen,\n",
            "Prack asself’s Offfet worll.\n",
            "\n",
            "CADIARERIA.\n",
            "Year eve?\n",
            "\n",
            "Hirg.\n",
            "Harthous\n",
            "anst bold freens to dike hears gup richohety,\n",
            "The arm, sir™ond that, man hand her me not the fatherity.\n",
            "When he looke with. May for mores\n",
            "And thee Edangland logess—hishing, that rightal amused. [_Agalles’ here oit or my lack of lord.\n",
            "\n",
            "ALFELIO.\n",
            "He not ghoners. Hursty.\n",
            "\n",
            "BANDERIO.\n",
            "Lived no at he shand the jusesters;\n",
            "For you and her, dierer.\n",
            "\n",
            "SENICLA.\n",
            "A’SASALO.\n",
            "My well; merellia wors. Yorly\n",
            "Someturenny the seent ell emony the grate?\n",
            "That to of blaing froms, my borth, lest,\n",
            "Let, emectain indeat that thou horservy that shame? Go ’tices;\n",
            "And condout reath, your will and has. Go here and by nit\n",
            "that him his borame wathing—Ha, I, darwi8]\n",
            "\n",
            "PAIRWAGUS.\n",
            "Whut to a sped geoce and them\n",
            "dot?\n",
            "\n",
            "KINT RIAGH, He mils ilnemer that Doiselvanceron eyentes.\n",
            "\n",
            "Enter Grepwise, or nepphit, for\n",
            "Of wasDigned.shance trumt Is.\n",
            "My\n",
            "ICHARNS.\n",
            "Ay, now a more you day unporter’st all for it the Orturs leectied,\n",
            "Mandus in things march up of grercereas\n",
            "Er God they all four I not as on that own.\n",
            "This vinice, luck.\n",
            "\n",
            "VISTER.\n",
            "Yenerius in eid efech on\n",
            "fillow morge’s go\n",
            "I great,\n",
            "Doojom I come, refwer as how wear.\n",
            "Whun we so man a pear it of of\n",
            "Posicess?Æ. A Jow your helts Edymos Packe.\n",
            "Should be in youingzore on exconards?_]\n",
            "\n",
            "TIC4 nothing th\n",
            "\n",
            "Freent the men the dath fathor_pition comstation unto let\n",
            "Go dost the to merron’d agettue of a shall finds a fufffo\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DONE!"
      ],
      "metadata": {
        "id": "jySPjB4rF_2I"
      }
    }
  ]
}