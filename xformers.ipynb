{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMWNYQAXVLoIPZXcSPrt1r7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sheneman/shakeGPT/blob/main/xformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QWnXAHGSZrAl"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "###########################################################\n",
        "#\n",
        "# ShakeGPT - HuggingFace Transformers Library\n",
        "#\n",
        "# Learning to emit endless Shakespeare to demonstrate how\n",
        "# decoder-only next-token prediction transformer models\n",
        "# work.  In this case we use the transformers library.\n",
        "#\n",
        "# Luke Sheneman\n",
        "# sheneman@uidaho.edu\n",
        "# January 2024\n",
        "#\n",
        "# Research Computing and Data Services (RCDS)\n",
        "# Insitite for Interdisciplinary Data Sciences (IIDS)\n",
        "# University of Idaho\n",
        "#\n",
        "##########################################################\n",
        "\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config, TextDataset, DataCollatorForLanguageModeling\n",
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "# Our Hyperparamters\n",
        "INPUT_CORPUS     = \"complete_shakespeare.txt\"\n",
        "OUTDIR           = \"shakespeare_output\"\n",
        "LOGDIR           = \"shakespeare_logs\"\n",
        "\n",
        "LOGGING_STEPS    = 10\n",
        "BATCH_SIZE       = 16\n",
        "EPOCHS\t\t = 100\n",
        "CONTEXT_WINDOW   = 32\n",
        "\n",
        "# Initialize a GPT-2 model with default parameters\n",
        "config = GPT2Config()\n",
        "model = GPT2LMHeadModel(config)\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Create the dataset\n",
        "train_dataset = TextDataset(\n",
        "\ttokenizer=tokenizer,\n",
        "\tfile_path=INPUT_CORPUS,\n",
        "\tblock_size=128\n",
        ")\n",
        "\n",
        "# Create a data collator for dynamic padding\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "\ttokenizer=tokenizer, mlm=False\n",
        ")\n",
        "\n",
        "# Define the training arguments\n",
        "training_args = TrainingArguments(\n",
        "\toutput_dir=OUTDIR,\n",
        "\tnum_train_epochs=EPOCHS,\n",
        "\tper_device_train_batch_size=BATCH_SIZE,\n",
        "\tlogging_dir=LOGDIR,\n",
        "\tlogging_steps=LOGGING_STEPS,\n",
        ")\n",
        "\n",
        "# Bow to your sensei\n",
        "trainer = Trainer(\n",
        "\tmodel=model,\n",
        "\targs=training_args,\n",
        "\tdata_collator=data_collator,\n",
        "\ttrain_dataset=train_dataset\n",
        ")\n",
        "\n",
        "# Function to generate text\n",
        "def generate_text(prompt, max_length=CONTEXT_WINDOW):\n",
        "\tinput_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
        "\n",
        "\t# Generate text using the model\n",
        "\toutput = model.generate(input_ids, max_length=max_length, num_return_sequences=1, no_repeat_ngram_size=2)\n",
        "\n",
        "\t# Decode and return the generated text\n",
        "\treturn tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "# Now let's perform some inference!\n",
        "prompt = \"To be, or not to be\"\n",
        "generated_text = generate_text(prompt)\n",
        "print(generated_text)"
      ],
      "metadata": {
        "id": "fkZVv5j-Zr7T"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}